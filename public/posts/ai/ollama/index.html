<!DOCTYPE html>
<html lang="ko" dir="auto">

<head><script src="/portfolio/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=portfolio/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Ollama | RyanLabs</title>
<meta name="keywords" content="Ollama, AI, Llama">
<meta name="description" content="EC2의 Ubuntu 인스턴스에서 Ollama를 설치하고 사용하는 방법
EC2의 Ubuntu 인스턴스에서 Ollama를 설치하고 사용하는 방법을 안내해드리겠습니다.
1. Ollama 설치
Ollama는 로컬 환경에서 대규모 언어 모델(LLM)을 실행할 수 있게 해주는 오픈 소스 프레임워크입니다. Ubuntu에서 Ollama를 설치하려면 다음 단계를 따르세요.


시스템 업데이트 및 필수 패키지 설치:
sudo apt update &amp;&amp; sudo apt upgrade -y
sudo apt install -y curl


Ollama 설치 스크립트 실행:
curl -fsSL https://ollama.com/install.sh | sh


설치 확인:
ollama --version
위 명령어를 실행하여 Ollama가 정상적으로 설치되었는지 확인합니다.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/portfolio/posts/ai/ollama/">
<link crossorigin="anonymous" href="/portfolio/assets/css/stylesheet.6da9a63d25a9608bca2f7f907a030e887a7dd3c3f3918e4cc113129361414bda.css" integrity="sha256-bammPSWpYIvKL3&#43;QegMOiHp908PzkY5MwRMSk2FBS9o=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/portfolio/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/portfolio/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/portfolio/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/portfolio/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/portfolio/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="ko" href="http://localhost:1313/portfolio/posts/ai/ollama/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/portfolio/" accesskey="h" title="RyanLabs (Alt + H)">RyanLabs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/portfolio/search/" title="검색">
                    <span>검색</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/portfolio/categories/" title="카테고리">
                    <span>카테고리</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/portfolio/tags/" title="태그">
                    <span>태그</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/portfolio/archives/" title="연도">
                    <span>연도</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/portfolio/lectures/" title="강의">
                    <span>강의</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/portfolio/about/" title="어바웃미">
                    <span>어바웃미</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Ollama
    </h1>
    <div class="post-meta"><span title='2025-02-17 00:00:00 +0000 UTC'>2월 17, 2025</span>

</div>
  </header> 
  <div class="post-content"><h1 id="ec2의-ubuntu-인스턴스에서-ollama를-설치하고-사용하는-방법">EC2의 Ubuntu 인스턴스에서 Ollama를 설치하고 사용하는 방법<a hidden class="anchor" aria-hidden="true" href="#ec2의-ubuntu-인스턴스에서-ollama를-설치하고-사용하는-방법">#</a></h1>
<p>EC2의 Ubuntu 인스턴스에서 Ollama를 설치하고 사용하는 방법을 안내해드리겠습니다.</p>
<h3 id="1-ollama-설치">1. Ollama 설치<a hidden class="anchor" aria-hidden="true" href="#1-ollama-설치">#</a></h3>
<p>Ollama는 로컬 환경에서 대규모 언어 모델(LLM)을 실행할 수 있게 해주는 오픈 소스 프레임워크입니다. Ubuntu에서 Ollama를 설치하려면 다음 단계를 따르세요.</p>
<ol>
<li>
<p><strong>시스템 업데이트 및 필수 패키지 설치:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt update <span style="color:#f92672">&amp;&amp;</span> sudo apt upgrade -y
</span></span><span style="display:flex;"><span>sudo apt install -y curl
</span></span></code></pre></div></li>
<li>
<p><strong>Ollama 설치 스크립트 실행:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -fsSL https://ollama.com/install.sh | sh
</span></span></code></pre></div></li>
<li>
<p><strong>설치 확인:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama --version
</span></span></code></pre></div><p>위 명령어를 실행하여 Ollama가 정상적으로 설치되었는지 확인합니다.</p>
</li>
</ol>
<h3 id="2-ollama-서비스-시작-및-종료">2. Ollama 서비스 시작 및 종료<a hidden class="anchor" aria-hidden="true" href="#2-ollama-서비스-시작-및-종료">#</a></h3>
<p>Ollama는 모델을 제공하기 위해 서버 모드로 실행할 수 있습니다.</p>
<ul>
<li>
<p><strong>서비스 시작:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama serve
</span></span></code></pre></div><p>이 명령어는 Ollama 서버를 시작하여 모델을 API 형태로 제공하게 합니다.</p>
</li>
<li>
<p><strong>서비스 종료:</strong></p>
<p><code>serve</code> 명령어는 포그라운드에서 실행되므로, 터미널에서 <code>Ctrl + C</code>를 눌러 종료할 수 있습니다. 백그라운드에서 실행 중인 경우, <code>ps</code> 명령어를 사용하여 프로세스 ID를 확인한 후 <code>kill</code> 명령어로 종료할 수 있습니다.</p>
</li>
</ul>
<h3 id="3-ollama-명령어">3. Ollama 명령어<a hidden class="anchor" aria-hidden="true" href="#3-ollama-명령어">#</a></h3>
<p>Ollama는 다양한 명령어를 제공합니다. 주요 명령어는 다음과 같습니다.</p>
<ul>
<li>
<p><strong>모델 실행:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run &lt;모델_이름&gt;
</span></span></code></pre></div><p>예를 들어, <code>llama3</code> 모델을 실행하려면 다음과 같이 입력합니다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run llama3
</span></span></code></pre></div></li>
<li>
<p><strong>모델 다운로드:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama pull &lt;모델_이름&gt;
</span></span></code></pre></div><p>특정 모델을 사전에 다운로드하려면 위 명령어를 사용합니다.</p>
</li>
<li>
<p><strong>로컬에 설치된 모델 목록 확인:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama list
</span></span></code></pre></div><p>현재 시스템에 설치된 모델들을 확인할 수 있습니다.</p>
</li>
<li>
<p><strong>모델 정보 확인:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama show &lt;모델_이름&gt;
</span></span></code></pre></div><p>특정 모델의 상세 정보를 확인합니다.</p>
</li>
</ul>
<h3 id="4-모델-저장-위치-변경">4. 모델 저장 위치 변경<a hidden class="anchor" aria-hidden="true" href="#4-모델-저장-위치-변경">#</a></h3>
<p>Ollama는 기본적으로 모델을 시스템의 특정 디렉토리에 저장합니다. 그러나 저장 공간 관리나 다른 디스크를 활용하기 위해 모델 저장 위치를 변경할 수 있습니다.</p>
<ol>
<li>
<p><strong>환경 변수 설정:</strong></p>
<p>모델을 저장할 디렉토리를 생성한 후, <code>OLLAMA_MODELS</code> 환경 변수를 설정합니다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>export OLLAMA_MODELS<span style="color:#f92672">=</span>/path/to/your/preferred/directory
</span></span></code></pre></div><p>이 설정을 영구적으로 적용하려면 해당 내용을 <code>~/.bashrc</code> 또는 <code>~/.bash_profile</code>에 추가한 후 <code>source</code> 명령어로 적용합니다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>echo <span style="color:#e6db74">&#39;export OLLAMA_MODELS=/path/to/your/preferred/directory&#39;</span> &gt;&gt; ~/.bashrc
</span></span><span style="display:flex;"><span>source ~/.bashrc
</span></span></code></pre></div></li>
<li>
<p><strong>서버 실행 시 적용:</strong></p>
<p>환경 변수를 설정한 후, Ollama 서버를 실행하면 모델이 지정한 디렉토리에 저장됩니다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama serve
</span></span></code></pre></div><p>또는 서버 실행 시 직접 환경 변수를 지정할 수도 있습니다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>OLLAMA_MODELS<span style="color:#f92672">=</span>/path/to/your/preferred/directory ollama serve
</span></span></code></pre></div></li>
</ol>
<p>이러한 설정을 통해 모델 저장 위치를 원하는 디렉토리로 변경할 수 있습니다.</p>
<p>위의 절차를 따르면 EC2의 Ubuntu 인스턴스에서 Ollama를 설치하고 효율적으로 사용할 수 있습니다.</p>
<h3 id="5-실제-모델-저장위치-방법">5. 실제 모델 저장위치 방법<a hidden class="anchor" aria-hidden="true" href="#5-실제-모델-저장위치-방법">#</a></h3>
<p>중요한 것은 models 폴더를 symbol 링크를 만들어 저장 폴더를 가르키게 하고, owner를 ubuntu로 변경하는 것이다.
그리고 /etc/systemd/system/ollama.service 에서도 user / group 을 ubuntu로 변경해서 정상동작한다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ ll
</span></span><span style="display:flex;"><span>total <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>drwxr-xr-x  <span style="color:#ae81ff">2</span> ubuntu ubuntu <span style="color:#ae81ff">4096</span> Feb <span style="color:#ae81ff">18</span> 02:09 ./
</span></span><span style="display:flex;"><span>drwx------ <span style="color:#ae81ff">30</span> ubuntu ubuntu <span style="color:#ae81ff">4096</span> Feb <span style="color:#ae81ff">18</span> 02:02 ../
</span></span><span style="display:flex;"><span>-rw-------  <span style="color:#ae81ff">1</span> ubuntu ubuntu  <span style="color:#ae81ff">896</span> Feb <span style="color:#ae81ff">18</span> 02:09 history
</span></span><span style="display:flex;"><span>-rw-------  <span style="color:#ae81ff">1</span> ubuntu ubuntu  <span style="color:#ae81ff">387</span> Jan <span style="color:#ae81ff">24</span> 10:31 id_ed25519
</span></span><span style="display:flex;"><span>-rw-r--r--  <span style="color:#ae81ff">1</span> ubuntu ubuntu   <span style="color:#ae81ff">81</span> Jan <span style="color:#ae81ff">24</span> 10:31 id_ed25519.pub
</span></span><span style="display:flex;"><span>lrwxrwxrwx  <span style="color:#ae81ff">1</span> ubuntu ubuntu   <span style="color:#ae81ff">37</span> Feb <span style="color:#ae81ff">18</span> 01:40 models -&gt; /home/ubuntu/workspace/.ollama/models/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ ll /usr/share/ollama/.ollama/models
</span></span><span style="display:flex;"><span>lrwxrwxrwx <span style="color:#ae81ff">1</span> ollama ollama <span style="color:#ae81ff">37</span> Jan <span style="color:#ae81ff">24</span> 10:23 /usr/share/ollama/.ollama/models -&gt; /home/ubuntu/workspace/.ollama/models/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ ll ~/.ollama/models
</span></span><span style="display:flex;"><span>lrwxrwxrwx <span style="color:#ae81ff">1</span> ubuntu ubuntu <span style="color:#ae81ff">37</span> Feb <span style="color:#ae81ff">18</span> 01:40 /home/ubuntu/.ollama/models -&gt; /home/ubuntu/workspace/.ollama/models/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ rm /usr/share/ollama/.ollama/models
</span></span><span style="display:flex;"><span>rm: cannot remove <span style="color:#e6db74">&#39;/usr/share/ollama/.ollama/models&#39;</span>: Permission denied
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ sudo rm /usr/share/ollama/.ollama/models
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ ollama run llama3.2
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; /bye
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ rm /usr/share/ollama/.ollama/models
</span></span><span style="display:flex;"><span>rm: cannot remove <span style="color:#e6db74">&#39;/usr/share/ollama/.ollama/models&#39;</span>: No such file or directory
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ rm /home/ubuntu/.ollama/models
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ ollama run llama3.2
</span></span><span style="display:flex;"><span>pulling manifest
</span></span><span style="display:flex;"><span>pulling dde5aa3fc5ff... 100% ▕█████████████████████████████████████▏ 2.0 GB
</span></span><span style="display:flex;"><span>pulling 966de95ca8a6... 100% ▕█████████████████████████████████████▏ 1.4 KB
</span></span><span style="display:flex;"><span>pulling fcc5a6bec9da... 100% ▕█████████████████████████████████████▏ 7.7 KB
</span></span><span style="display:flex;"><span>pulling a70ff7e570d9... 100% ▕█████████████████████████████████████▏ 6.0 KB
</span></span><span style="display:flex;"><span>pulling 56bb8bd477a5... 100% ▕█████████████████████████████████████▏   <span style="color:#ae81ff">96</span> B
</span></span><span style="display:flex;"><span>pulling 34bb5ab01051... 100% ▕█████████████████████████████████████▏  <span style="color:#ae81ff">561</span> B
</span></span><span style="display:flex;"><span>verifying sha256 digest
</span></span><span style="display:flex;"><span>writing manifest
</span></span><span style="display:flex;"><span>success
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; /bye
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ ll /home/ubuntu/.ollama/models
</span></span><span style="display:flex;"><span>total <span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ubuntu@ip-172-31-26-173:~/.ollama$ cat /etc/systemd/system/ollama.service
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>Unit<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>Description<span style="color:#f92672">=</span>Ollama Service
</span></span><span style="display:flex;"><span>After<span style="color:#f92672">=</span>network-online.target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>Service<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>ExecStart<span style="color:#f92672">=</span>/usr/local/bin/ollama serve
</span></span><span style="display:flex;"><span>User<span style="color:#f92672">=</span>ubuntu
</span></span><span style="display:flex;"><span>Greoup<span style="color:#f92672">=</span>ubuntu
</span></span><span style="display:flex;"><span><span style="color:#75715e"># User=ollama</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Group=ollama</span>
</span></span><span style="display:flex;"><span>Restart<span style="color:#f92672">=</span>always
</span></span><span style="display:flex;"><span>RestartSec<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>Environment<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;PATH=/home/ubuntu/anaconda3/bin:/home/ubuntu/anaconda3/condabin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/cuda-12.1/bin:/usr/local/cuda-12.1/include:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ryan add</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ProtectHome=no</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Environment=&#34;Home=/home/ubuntu/workspace/.ollama/&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Environment=&#34;OLLAMA_MODELS=/home/ubuntu/workspace/.ollama/models&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>Install<span style="color:#f92672">]</span>
</span></span></code></pre></div><hr>
<h1 id="best-gui">Best GUI<a hidden class="anchor" aria-hidden="true" href="#best-gui">#</a></h1>
<p>Below is a detailed comparison of three GUI tools—LM Studio, Open WebUI, and Gradio—for integrating with a Retrieval-Augmented Generation (RAG) workflow, specifically for deployment on an EC2 Ubuntu instance.</p>
<hr>
<h3 id="1-lm-studio">1. LM Studio<a hidden class="anchor" aria-hidden="true" href="#1-lm-studio">#</a></h3>
<p><strong>Overview:</strong><br>
LM Studio is a dedicated local inference platform designed to manage and run large language models (LLMs) via a standalone desktop-style interface.</p>
<p><strong>Strengths:</strong></p>
<ul>
<li><strong>Model Management:</strong> Provides out‐of‐the‐box features for downloading, updating, and managing local LLMs.</li>
<li><strong>Integrated Inference Server:</strong> Offers a built-in inference server that can process model requests locally.</li>
<li><strong>User-Friendly Interface:</strong> Its GUI is designed for ease of use, making it accessible to users who prefer a desktop-like experience.</li>
</ul>
<p><strong>Limitations for RAG:</strong></p>
<ul>
<li><strong>RAG-Specific Integration:</strong> LM Studio isn’t specifically built for integrating retrieval components with generation. If your workflow requires dynamic document retrieval and on-the-fly augmentation of model responses, you might need to build additional custom logic.</li>
<li><strong>Deployment Flexibility:</strong> While it runs well as a standalone application, adapting it for web-based or multi-user RAG deployments on a server might require extra configuration.</li>
</ul>
<p><strong>Deployment on EC2 Ubuntu:</strong></p>
<ul>
<li>LM Studio can be installed on Ubuntu (using an AppImage or installation package) and run on EC2. However, you may need to configure it to run headless or as a background service if you plan to access it via a web interface.</li>
</ul>
<hr>
<h3 id="2-open-webui">2. Open WebUI<a hidden class="anchor" aria-hidden="true" href="#2-open-webui">#</a></h3>
<p><strong>Overview:</strong><br>
Open WebUI is a web-based interface that supports interacting with various LLMs. It’s designed to be extensible and can integrate document retrieval features essential for RAG workflows.</p>
<p><strong>Strengths:</strong></p>
<ul>
<li><strong>Web-Based Interface:</strong> Accessible via a browser, making it ideal for multi-user and remote deployments.</li>
<li><strong>RAG Integration:</strong> Built-in support for integrating with document libraries and retrieval systems—users can upload documents and have the model generate responses based on both the conversation and retrieved content.</li>
<li><strong>Container-Friendly:</strong> Often deployed via Docker, which simplifies installation and scaling on EC2 Ubuntu.</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li><strong>Installation Complexity:</strong> While Docker simplifies deployment, initial setup may require familiarity with containerization.</li>
<li><strong>Customization:</strong> For very specific use cases, you may need to tweak configuration files or code.</li>
</ul>
<p><strong>Deployment on EC2 Ubuntu:</strong></p>
<ul>
<li>Open WebUI is commonly deployed using Docker. With Docker installed on your EC2 Ubuntu instance, you can pull the official image and run it with a few commands, exposing the service on a desired port (e.g., 3000).</li>
</ul>
<p><strong>RAG Integration:</strong></p>
<ul>
<li>Open WebUI excels at RAG tasks due to its ability to incorporate document search and retrieval modules alongside model inference, enabling richer, context-aware interactions.</li>
</ul>
<hr>
<h3 id="3-gradio">3. Gradio<a hidden class="anchor" aria-hidden="true" href="#3-gradio">#</a></h3>
<p><strong>Overview:</strong><br>
Gradio is a Python library that lets you build interactive web UIs quickly for machine learning models. It’s widely used for demos and prototyping.</p>
<p><strong>Strengths:</strong></p>
<ul>
<li><strong>Ease of Use:</strong> Very simple to set up; you write a few lines of Python to create an interactive interface.</li>
<li><strong>Flexibility:</strong> Highly customizable if you’re comfortable with Python. You can integrate any retrieval mechanism you build yourself.</li>
<li><strong>Rapid Prototyping:</strong> Ideal for quick demos, experiments, or proof-of-concept RAG applications.</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li><strong>Production Readiness:</strong> While Gradio is excellent for demos and prototyping, it might require additional effort to harden it for production use (e.g., reverse proxy, security configurations).</li>
<li><strong>Manual Integration:</strong> RAG functionality (retrieval plus generation) isn’t built-in; you need to implement the retrieval logic separately and wire it to the UI.</li>
</ul>
<p><strong>Deployment on EC2 Ubuntu:</strong></p>
<ul>
<li>You can install Gradio via pip and run your Python scripts on EC2 Ubuntu. The interface will run on a specified port, which you can expose using a reverse proxy (like Nginx) if needed.</li>
</ul>
<p><strong>RAG Integration:</strong></p>
<ul>
<li>With Gradio, you have full control over the integration. You can build your retrieval system in Python, then create an interface where the model’s responses are augmented by retrieved documents. However, this requires custom coding.</li>
</ul>
<hr>
<h2 id="open-webui와-lm-studio-비교">Open WebUI와 LM Studio 비교<a hidden class="anchor" aria-hidden="true" href="#open-webui와-lm-studio-비교">#</a></h2>
<p>Open WebUI와 LM Studio는 모두 로컬 환경에서 대규모 언어 모델(LLM)을 활용할 수 있는 도구로, Retrieval-Augmented Generation(RAG) 기능과의 통합 측면에서 각각의 특징을 비교해보겠습니다.</p>
<h3 id="open-webui">Open WebUI<a hidden class="anchor" aria-hidden="true" href="#open-webui">#</a></h3>
<p><strong>RAG 통합 기능:</strong></p>
<ul>
<li>
<p><strong>문서 기반 대화:</strong> Open WebUI는 로컬에 저장된 PDF나 TXT 파일을 업로드하여, 해당 문서의 내용을 기반으로 모델과 상호작용할 수 있는 RAG 기능을 제공합니다. 이를 통해 사용자는 개인 문서를 모델과의 대화에 직접 활용할 수 있습니다. citeturn0search1</p>
</li>
<li>
<p><strong>웹 검색 통합:</strong> SearXNG, Google PSE, Brave Search, DuckDuckGo 등 다양한 검색 엔진과의 통합을 통해 실시간 웹 검색 결과를 대화에 포함시킬 수 있습니다. 이를 통해 최신 정보를 모델 응답에 반영할 수 있습니다. citeturn0search9</p>
</li>
</ul>
<p><strong>사용자 경험:</strong></p>
<ul>
<li>
<p><strong>커스터마이즈 가능성:</strong> Open WebUI는 다양한 플러그인과 설정을 통해 사용자 맞춤형 기능을 제공하며, 고급 사용자에게 적합한 높은 수준의 커스터마이즈를 지원합니다.</p>
</li>
<li>
<p><strong>설치 및 설정:</strong> Docker를 활용한 설치를 권장하며, 설치 및 초기 설정에는 다소 기술적인 지식이 필요할 수 있습니다.</p>
</li>
</ul>
<h3 id="lm-studio">LM Studio<a hidden class="anchor" aria-hidden="true" href="#lm-studio">#</a></h3>
<p><strong>RAG 통합 기능:</strong></p>
<ul>
<li>
<p><strong>모델 관리 및 실행:</strong> LM Studio는 다양한 LLM 모델을 로컬에서 관리하고 실행할 수 있는 플랫폼으로, 내장된 추론 서버를 통해 모델을 효율적으로 활용할 수 있습니다.</p>
</li>
<li>
<p><strong>RAG 구현:</strong> LM Studio 자체에는 직접적인 RAG 기능이 내장되어 있지 않지만, 내장된 추론 서버를 통해 외부 애플리케이션이나 인터페이스와 연동하여 RAG 기능을 구현할 수 있습니다.</p>
</li>
</ul>
<p><strong>사용자 경험:</strong></p>
<ul>
<li>
<p><strong>직관적인 인터페이스:</strong> LM Studio는 사용하기 쉬운 인터페이스를 제공하여, 개발자뿐만 아니라 일반 사용자도 손쉽게 모델을 관리하고 활용할 수 있습니다.</p>
</li>
<li>
<p><strong>설치 및 설정:</strong> 독립 실행형 애플리케이션으로 제공되어, 설치 과정이 비교적 간단하며, 추가적인 설정 없이 바로 사용할 수 있습니다.</p>
</li>
</ul>
<h3 id="결론">결론<a hidden class="anchor" aria-hidden="true" href="#결론">#</a></h3>
<ul>
<li>
<p><strong>Open WebUI:</strong> 로컬 문서와의 상호작용 및 실시간 웹 검색 통합 등 강력한 RAG 기능을 내장하고 있으며, 다양한 커스터마이즈 옵션을 통해 고급 사용자의 요구를 충족시킬 수 있습니다.</p>
</li>
<li>
<p><strong>LM Studio:</strong> 직관적인 인터페이스와 간편한 설치 과정을 통해 사용 편의성을 제공하며, 외부 애플리케이션과의 연동을 통해 RAG 기능을 구현할 수 있습니다.</p>
</li>
</ul>
<p>따라서, 사용자의 기술 수준과 요구 사항에 따라 Open WebUI는 고급 기능과 커스터마이즈를 원하는 사용자에게, LM Studio는 간편한 설치와 사용을 원하는 사용자에게 적합한 선택이 될 수 있습니다.</p>
<hr>
<h2 id="final-recommendation-for-rag-ui-on-ec2-ubuntu">Final Recommendation for RAG UI on EC2 Ubuntu<a hidden class="anchor" aria-hidden="true" href="#final-recommendation-for-rag-ui-on-ec2-ubuntu">#</a></h2>
<ul>
<li>
<p><strong>For a robust, production-ready web interface with built-in retrieval support, <em>Open WebUI</em> is likely the best choice.</strong> It offers a complete web solution with integrated document libraries and search capabilities, making it ideal for a Retrieval-Augmented Generation workflow.</p>
</li>
<li>
<p><strong>If rapid prototyping and flexibility are your primary goals, <em>Gradio</em> is a strong candidate.</strong> It allows you to quickly develop a custom UI that integrates your retrieval logic with model inference, though you’ll need to build more of the backend logic yourself.</p>
</li>
<li>
<p><strong>LM Studio</strong> is excellent for managing and running local language models, but it might not offer the out-of-the-box RAG integration that you need, and adapting it for multi-user or web-based RAG deployments on EC2 Ubuntu could require additional work.</p>
</li>
</ul>
<p>Please let us know if you need further details or assistance with setting up any of these tools.</p>
<hr>
<p><a href="https://www.restack.io/p/anything-llm-answer-lm-studio-vs-open-webui-cat-ai">https://www.restack.io/p/anything-llm-answer-lm-studio-vs-open-webui-cat-ai</a></p>
<p>Performance and Scalability
LMStudio is optimized for local deployments, ensuring that users can leverage their hardware effectively. The built-in inference server allows for quick responses and efficient resource management.
Open WebUI is designed for cloud-based applications, which can offer scalability but may introduce latency depending on the network conditions.</p>
<p>Conclusion
In summary, the choice between LMStudio and Open WebUI largely depends on the user&rsquo;s specific needs and preferences. For those prioritizing ease of use and local performance, LMStudio is an excellent choice. Conversely, users looking for customization and cloud capabilities may prefer Open WebUI.</p>
<hr>
<h2 id="open-webui와-gradio-비교">Open WebUI와 Gradio 비교<a hidden class="anchor" aria-hidden="true" href="#open-webui와-gradio-비교">#</a></h2>
<p>아래는 EC2 Ubuntu에서 실행할 RAG(Retrieval-Augmented Generation) 시스템 구축 시, Open WebUI와 Gradio를 RAG 지원 측면에서 비교한 내용을 한국어로 번역한 것입니다.</p>
<hr>
<h3 id="open-webui-1"><strong>Open WebUI</strong><a hidden class="anchor" aria-hidden="true" href="#open-webui-1">#</a></h3>
<p><strong>1. 목적 및 RAG에 대한 내장 기능:</strong></p>
<ul>
<li>
<p><strong>문서 수집 및 인덱싱:</strong><br>
Open WebUI는 로컬 문서 라이브러리와의 연동이 원활하도록 설계되어 있습니다. PDF, HTML, TXT 등의 문서를 업로드, 인덱싱, 그리고 검색할 수 있는 기능을 기본으로 제공하여, RAG 시스템에서 외부 문서의 내용을 검색하고 모델 응답에 반영할 수 있습니다.</p>
</li>
<li>
<p><strong>통합 RAG 워크플로우:</strong><br>
Open WebUI는 검색된 문서의 정보를 모델 추론과 결합하는 설정이나 플러그인을 제공하여, 대화형 인터페이스 내에서 retrieval과 generation을 함께 수행할 수 있습니다.</p>
</li>
<li>
<p><strong>멀티유저 웹 인터페이스:</strong><br>
웹 기반으로 동작하기 때문에 여러 사용자가 동시에 접근할 수 있으며, Docker를 통해 배포할 수 있어 환경 일관성을 유지하면서 확장성 있게 운영할 수 있습니다.</p>
</li>
</ul>
<p><strong>2. 배포 및 사용자 정의:</strong></p>
<ul>
<li><strong>Docker 기반 배포:</strong><br>
Docker를 사용하면 EC2 Ubuntu 환경에 쉽게 배포할 수 있으며, 설정이 표준화되어 있어 관리가 용이합니다.</li>
<li><strong>커스터마이즈:</strong><br>
플러그인과 설정 옵션이 다양하여 검색, 인덱싱, 문서 처리 등 RAG에 필요한 맞춤형 기능을 쉽게 추가할 수 있습니다.</li>
<li><strong>성능 및 확장성:</strong><br>
외부 벡터 데이터베이스나 검색 엔진과의 연동을 통해 대량의 문서 검색 작업을 효율적으로 처리할 수 있습니다.</li>
</ul>
<hr>
<h3 id="gradio"><strong>Gradio</strong><a hidden class="anchor" aria-hidden="true" href="#gradio">#</a></h3>
<p><strong>1. 목적 및 RAG에 대한 기능:</strong></p>
<ul>
<li><strong>빠른 프로토타이핑:</strong><br>
Gradio는 Python 라이브러리로, 몇 줄의 코드로 인터랙티브한 웹 인터페이스를 쉽게 만들 수 있어, RAG 시스템의 초기 프로토타입 제작에 매우 유용합니다.</li>
<li><strong>UI 구성의 유연성:</strong><br>
사용자가 원하는 방식으로 인터페이스를 자유롭게 설계할 수 있으므로, 파일 업로드, 문서 처리, 그리고 모델 응답 결합을 위한 맞춤형 UI를 만들 수 있습니다.</li>
<li><strong>내장 기능 제한:</strong><br>
Gradio는 기본적으로 문서 수집이나 인덱싱, 검색 기능을 제공하지 않으므로, RAG 시스템에 필요한 문서 처리 파이프라인(예: Apache Tika, PDFMiner, 벡터 데이터베이스 연동 등)을 직접 구현해야 합니다.</li>
</ul>
<p><strong>2. 배포 및 사용자 정의:</strong></p>
<ul>
<li><strong>설정의 간단함:</strong><br>
pip로 설치가 간편하며, Python 스크립트를 실행하여 인터페이스를 구동할 수 있으므로 초기 개발 및 데모용으로 매우 적합합니다.</li>
<li><strong>프로덕션 준비도:</strong><br>
Gradio는 데모와 프로토타이핑에 최적화되어 있어, 생산 환경에 사용하려면 추가적인 보안, 성능 최적화, 리버스 프록시 설정 등이 필요할 수 있습니다.</li>
<li><strong>수동 통합:</strong><br>
RAG 시스템에 필요한 문서 처리, 검색, 인덱싱 로직을 별도로 구현한 후, Gradio 인터페이스에 연결해야 합니다.</li>
</ul>
<hr>
<h3 id="rag-시스템-구축-시-추천"><strong>RAG 시스템 구축 시 추천</strong><a hidden class="anchor" aria-hidden="true" href="#rag-시스템-구축-시-추천">#</a></h3>
<ul>
<li>
<p><strong>Open WebUI</strong>:<br>
RAG 시스템에서 문서 수집, 인덱싱, 검색 등 retrieval 기능이 내장되어 있고, Docker를 통한 배포와 다중 사용자 지원, 확장성 측면에서 강력한 기능을 제공하므로, 생산 환경의 RAG 시스템 구축에 적합합니다.</p>
</li>
<li>
<p><strong>Gradio</strong>:<br>
빠른 프로토타이핑과 인터랙티브한 데모 제작에 유리하지만, RAG에 필요한 문서 처리 및 검색 기능은 직접 구현해야 하므로, 프로토타입 단계에서는 좋으나, 완성도 높은 프로덕션 시스템을 구축하기 위해서는 추가적인 개발이 필요합니다.</p>
</li>
</ul>
<hr>
<h3 id="요약"><strong>요약</strong><a hidden class="anchor" aria-hidden="true" href="#요약">#</a></h3>
<ul>
<li><strong>RAG 통합 기능 측면에서 Open WebUI는</strong> 문서 업로드, 인덱싱, 검색 기능 등 RAG에 필요한 주요 기능들을 내장하고 있어, EC2 Ubuntu에서 RAG 시스템을 구축할 때 더 강력한 옵션입니다.</li>
<li><strong>Gradio는</strong> 간단한 UI와 빠른 개발 속도를 제공하지만, RAG 시스템에 필요한 문서 처리 및 retrieval 기능을 자체적으로 구현해야 하므로, 빠른 데모 제작에는 좋으나 프로덕션 단계에서는 추가 개발이 필요합니다.</li>
</ul>
<p>이 정보를 참고하여, 귀하의 RAG 시스템 구축 요구 사항에 가장 적합한 도구를 선택하시기 바랍니다. 추가 문의 사항이 있으면 언제든지 연락 주십시오.</p>
<hr>
<h1 id="open-webui-설치">Open WebUI 설치<a hidden class="anchor" aria-hidden="true" href="#open-webui-설치">#</a></h1>
<hr>
<p>Open WebUI는 사용자 친화적인 AI 인터페이스로, Docker를 통해 손쉽게 설치하고 실행할 수 있습니다. 아래에 Docker를 활용한 설치 방법과 Docker 없이 사용하는 방법을 안내해 드리겠습니다.</p>
<h3 id="docker를-사용하여-open-webui-설치-및-실행하기">Docker를 사용하여 Open WebUI 설치 및 실행하기<a hidden class="anchor" aria-hidden="true" href="#docker를-사용하여-open-webui-설치-및-실행하기">#</a></h3>
<ol>
<li>
<p><strong>Docker 설치:</strong></p>
<ul>
<li><strong>Ubuntu:</strong>
터미널을 열고 다음 명령어를 순차적으로 실행하여 Docker를 설치합니다:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt-get update
</span></span><span style="display:flex;"><span>sudo apt-get install ca-certificates curl
</span></span><span style="display:flex;"><span>sudo install -m <span style="color:#ae81ff">0755</span> -d /etc/apt/keyrings
</span></span><span style="display:flex;"><span>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo tee /etc/apt/keyrings/docker.asc
</span></span><span style="display:flex;"><span>sudo chmod a+r /etc/apt/keyrings/docker.asc
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;deb [arch=</span><span style="color:#66d9ef">$(</span>dpkg --print-architecture<span style="color:#66d9ef">)</span><span style="color:#e6db74"> signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu </span><span style="color:#66d9ef">$(</span>lsb_release -cs<span style="color:#66d9ef">)</span><span style="color:#e6db74"> stable&#34;</span> | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
</span></span><span style="display:flex;"><span>sudo apt-get update
</span></span><span style="display:flex;"><span>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
</span></span></code></pre></div>설치 완료 후, Docker가 정상적으로 설치되었는지 확인하려면 다음 명령어를 실행하세요:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo docker run hello-world
</span></span></code></pre></div>이 명령어를 실행하면 Docker가 정상적으로 작동하는지 확인할 수 있습니다.</li>
</ul>
</li>
<li>
<p><strong>Open WebUI 컨테이너 실행:</strong></p>
<ul>
<li>Docker를 통해 Open WebUI를 실행하려면 다음 명령어를 입력합니다:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d -p 3000:8080 --add-host<span style="color:#f92672">=</span>host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><ul>
<li><code>-d</code>: 백그라운드에서 컨테이너를 실행합니다.</li>
<li><code>-p 3000:8080</code>: 호스트의 포트 3000을 컨테이너의 포트 8080에 매핑합니다.</li>
<li><code>--add-host=host.docker.internal:host-gateway</code>: 컨테이너에서 호스트의 네트워크에 접근할 수 있도록 설정합니다.</li>
<li><code>-v open-webui:/app/backend/data</code>: 컨테이너의 데이터를 호스트에 저장하여 데이터 지속성을 유지합니다.</li>
<li><code>--name open-webui</code>: 컨테이너의 이름을 <code>open-webui</code>로 지정합니다.</li>
<li><code>--restart always</code>: 컨테이너가 중지되더라도 자동으로 재시작되도록 설정합니다.</li>
<li><code>ghcr.io/open-webui/open-webui:main</code>: 사용할 Docker 이미지를 지정합니다.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Open WebUI 접속:</strong></p>
<ul>
<li>컨테이너가 정상적으로 실행되면, 웹 브라우저에서 <code>http://localhost:3000</code>으로 접속하여 Open WebUI를 사용할 수 있습니다.</li>
</ul>
</li>
</ol>
<h3 id="docker-없이-open-webui-사용하기">Docker 없이 Open WebUI 사용하기<a hidden class="anchor" aria-hidden="true" href="#docker-없이-open-webui-사용하기">#</a></h3>
<p>Docker를 사용하지 않고 Open WebUI를 설치하려면 Python 환경에서 직접 설치할 수 있습니다.</p>
<ol>
<li>
<p><strong>Python 및 pip 설치:</strong></p>
<ul>
<li>Python 3.11 이상이 필요합니다.</li>
<li><strong>Ubuntu:</strong>
터미널에서 다음 명령어를 실행하여 Python과 pip를 설치합니다:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt-get update
</span></span><span style="display:flex;"><span>sudo apt-get install python3.11 python3.11-venv python3.11-dev
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p><strong>가상 환경 생성 및 활성화:</strong></p>
<ul>
<li>프로젝트 디렉토리를 생성하고 이동한 후, 가상 환경을 생성하고 활성화합니다:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python3.11 -m venv venv
</span></span><span style="display:flex;"><span>source venv/bin/activate
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p><strong>Open WebUI 설치:</strong></p>
<ul>
<li>pip를 사용하여 Open WebUI를 설치합니다:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install open-webui
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p><strong>Open WebUI 실행:</strong></p>
<ul>
<li>설치 후, 다음 명령어로 Open WebUI를 실행합니다:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>open-webui serve
</span></span></code></pre></div></li>
<li>이후 웹 브라우저에서 <code>http://localhost:3000</code>으로 접속하여 Open WebUI를 사용할 수 있습니다.</li>
</ul>
</li>
</ol>
<p><strong>참고:</strong> Docker를 사용하면 설치 및 배포가 간편하며, 환경 설정이 표준화되어 있어 권장됩니다. 그러나 Docker를 사용하지 않는 방법도 공식적으로 지원되므로, 필요와 환경에 따라 선택하시면 됩니다.</p>
<p><strong>출처:</strong></p>
<ul>
<li><a href="https://github.com/open-webui/open-webui">Open WebUI 공식 GitHub 저장소</a></li>
<li><a href="https://docs.openwebui.com/getting-started/#manual-installation">Open WebUI 설치 가이드</a></li>
</ul>
<hr>
<h1 id="lm-studio를-설치">LM Studio를 설치<a hidden class="anchor" aria-hidden="true" href="#lm-studio를-설치">#</a></h1>
<p>EC2 Ubuntu 환경에서 LM Studio를 설치하려면 다음 단계를 따르세요:</p>
<ol>
<li>
<p><strong>LM Studio AppImage 다운로드:</strong></p>
<ul>
<li>LM Studio의 공식 웹사이트 <a href="https://lmstudio.ai/">https://lmstudio.ai/</a>에서 최신 버전의 AppImage 파일을 다운로드합니다.</li>
</ul>
</li>
<li>
<p><strong>AppImage 파일 실행 권한 부여:</strong></p>
<ul>
<li>다운로드한 파일에 실행 권한을 부여합니다:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>chmod +x ./LM_Studio-&lt;version&gt;.AppImage
</span></span></code></pre></div><ul>
<li><code>&lt;version&gt;</code>을 다운로드한 파일의 실제 버전으로 대체하세요.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>LM Studio 실행:</strong></p>
<ul>
<li>일부 Ubuntu 버전에서는 보안 샌드박스 설정으로 인해 실행 시 문제가 발생할 수 있습니다. 이러한 경우 <code>--no-sandbox</code> 옵션을 사용하여 실행합니다:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>./LM_Studio-&lt;version&gt;.AppImage --no-sandbox
</span></span></code></pre></div><ul>
<li>이 방법은 Ubuntu 24.04에서 발생하는 문제를 해결하는 데 도움이 됩니다. citeturn0search0</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>이러한 단계를 통해 EC2 Ubuntu 환경에서 LM Studio를 설치하고 실행할 수 있습니다.</p>
<p>더 자세한 설치 과정과 문제 해결 방법은 아래 영상을 참고하시기 바랍니다:</p>
<p>videoHow to Install LM Studio on Linux to Run Models Locallyturn0search3</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/portfolio/tags/ollama/">Ollama</a></li>
      <li><a href="http://localhost:1313/portfolio/tags/ai/">AI</a></li>
      <li><a href="http://localhost:1313/portfolio/tags/llama/">Llama</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/portfolio/">RyanLabs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
